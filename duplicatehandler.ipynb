{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm  # For progress bar visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_file(file_path):\n",
    "    \"\"\"Generate a hash for a given file.\"\"\"\n",
    "    hash_algo = hashlib.sha256()\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            while chunk := file.read(8192):\n",
    "                hash_algo.update(chunk)\n",
    "        return file_path, hash_algo.hexdigest()\n",
    "    except Exception as e:\n",
    "        # Return None to indicate failure\n",
    "        return file_path, None\n",
    "\n",
    "def handle_file_conflict(dst):\n",
    "    \"\"\"Handle file conflicts by renaming the destination file.\"\"\"\n",
    "    base, extension = os.path.splitext(dst)\n",
    "    counter = 1\n",
    "    new_dst = dst\n",
    "    while os.path.exists(new_dst):\n",
    "        new_dst = f\"{base}_{counter}{extension}\"\n",
    "        counter += 1\n",
    "    return new_dst\n",
    "\n",
    "def process_files(files_to_process, duplicate_folder):\n",
    "    \"\"\"Process files using multithreading, moving duplicates to the 'duplicated' folder.\"\"\"\n",
    "    file_hashes = {}\n",
    "    duplicates = []\n",
    "    total_size_saved = 0\n",
    "\n",
    "    os.makedirs(duplicate_folder, exist_ok=True)\n",
    "\n",
    "    with ThreadPoolExecutor() as executor, tqdm(total=len(files_to_process), desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "        futures = {executor.submit(hash_file, fp): fp for fp in files_to_process}\n",
    "        for future in as_completed(futures):\n",
    "            file_path, file_hash = future.result()\n",
    "            if file_hash:\n",
    "                if file_hash in file_hashes:\n",
    "                    try:\n",
    "                        # Move (cut) the duplicate file\n",
    "                        dst_path = os.path.join(duplicate_folder, os.path.basename(file_path))\n",
    "                        dst_path = handle_file_conflict(dst_path)\n",
    "                        shutil.move(file_path, dst_path)  # Move instead of copy\n",
    "                        total_size_saved += os.path.getsize(dst_path)\n",
    "                        duplicates.append(file_path)\n",
    "                    except PermissionError:\n",
    "                        print(f\"\\nPermission denied: {file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\nError moving file {file_path}: {e}\")\n",
    "                else:\n",
    "                    file_hashes[file_hash] = file_path\n",
    "            pbar.update(1)  # Update progress bar for each file processed\n",
    "    return duplicates, total_size_saved\n",
    "\n",
    "def main():\n",
    "    directory = input(\"Enter the directory to scan for duplicates: \")\n",
    "    duplicate_folder = os.path.join(directory, 'duplicated')\n",
    "    os.makedirs(duplicate_folder, exist_ok=True)\n",
    "\n",
    "    # Collect all files to process\n",
    "    files_to_process = [\n",
    "        os.path.join(root, filename)\n",
    "        for root, _, files in os.walk(directory)\n",
    "        for filename in files\n",
    "    ]\n",
    "\n",
    "    print(\"Processing files...\")\n",
    "    duplicates, total_size_saved = process_files(files_to_process, duplicate_folder)\n",
    "\n",
    "    print(\"\\nProcess Complete!\")\n",
    "    print(f\"Duplicates moved to '{duplicate_folder}'.\")\n",
    "    print(f\"Total duplicates found: {len(duplicates)}\")\n",
    "    print(f\"Total space saved: {total_size_saved / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next i want to add a basic UI - better logic for the duplicate file handler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new task that handle filtering images that contains humans "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
